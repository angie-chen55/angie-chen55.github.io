<h1 align="center">
  Angelica Chen
</h1>
<p align="center">
  angelica[dot]chen[at]nyu.edu | <a href="https://www.semanticscholar.org/author/Angelica-Chen/13336152">Semantic Scholar profile</a>
</p>

Hi! I'm a PhD student at NYU Center for Data Science in the [Machine Learning for Language](https://wp.nyu.edu/ml2/) group, advised by [Sam Bowman](https://cims.nyu.edu/~sbowman/) and [Kyunghyun Cho](https://kyunghyuncho.me/). I'm broadly interested in deep learning for natural language understanding, code generation, model robustness, and improved evaluation metrics for NLU models. I have also previously worked as a student researcher at Google Research (Jun-Dec. 2021) on streaming models for disfluency detection and at Google Brain (Jun. 2022-Mar. 2023) on LMs for neural architecture search (NAS).

Prior to NYU, I worked as a software engineer at Google and graduated with high honors from Princeton Computer Science. [Sebastian Seung](https://www.cs.princeton.edu/people/profile/sseung) advised my senior thesis at Princeton, for which I received an Outstanding Computer Science Thesis award.

Outside of my research, I enjoy running, baking more pastries than I can feasibly eat, and cooking. I also volunteer as a rape and domestic violence crisis counselor/victim advocate for the [NYC Crime Victims Treatment Center](https://www.cvtcnyc.org/) (at Lenox Health Greenwich Village and Brookdale Hospital) and a crisis counselor for [Crisis Text Line](https://www.crisistextline.org/).

<h2 align="center">
  Selected Papers
</h2>
For a more complete list, see my <a href="https://www.semanticscholar.org/author/Angelica-Chen/13336152">Semantic Scholar profile</a>. <br>
<br>
<b>Chen, Angelica</b>, David M. Dohan and David R. So. “EvoPrompting: Language Models for Code-Level Neural Architecture Search.” (2023). https://arxiv.org/abs/2302.14838. <br>
<br>

Korbak, Tomasz, Kejian Shi, <b>Angelica Chen</b>, Rasika Bhalerao, Christopher L. Buckley, Jason Phang, Sam Bowman and Ethan Perez. “Pretraining Language Models with Human Preferences.” (2023). https://arxiv.org/abs/2302.08582.pdf. <br>
<br>

<b>Chen, Angelica</b>, Victoria Zayats, Daniel David Walker and Dirk Ryan Padfield. “Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection.” Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (2022). https://www.aclanthology.org/2022.naacl-main.60.pdf. <br>
<br>

Phang, Jason, <b>Angelica Chen</b>, William Huang and Samuel R. Bowman. “Adversarially Constructed Evaluation Sets Are More Challenging, but May Not Be Fair.” Dynamic Adversarial Data Collection (DADC) Workshop at NAACL 2022. https://arxiv.org/abs/2111.08181. <br>
<br>

Scheurer, J'er'emy, Jon Ander Campos, Jun Shern Chan, <b>Angelica Chen</b>, Kyunghyun Cho and Ethan Perez. “Training Language Models with Natural Language Feedback.” Association for Computational Linguistics Workshop on Learning with Natural Language Supervision (2022). https://arxiv.org/abs/2204.14146. <br>
<br>

Parrish, Alicia, <b>Angelica Chen</b>, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut and Sam Bowman. “BBQ: A hand-built bias benchmark for question answering.” Findings of the Association for Computational Linguistics (2022). https://aclanthology.org/2022.findings-acl.165/. <br>
<br>

Shaw, Peter, Philip Massey, <b>Angelica Chen</b>, Francesco Piccinno and Yasemin Altun. “Generating Logical Forms from Graph Representations of Text and Entities.” Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (2019). https://aclanthology.org/P19-1010/. <br>
<br>

Paireau, Juliette<sup>\*</sup>, <b>Angelica Chen<sup>\*</sup></b>, Hélène Broutin, Bryan T. Grenfell and Nicole E. Basta. “Seasonal dynamics of bacterial meningitis: a time-series analysis.” The Lancet Global health (2016). https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(16)30064-X/fulltext. <br>
<br>
