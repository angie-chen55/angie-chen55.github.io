<p align="center">
  <a href="#papers">Selected Papers</a> | <a href="#talks">Invited Talks</a> | <a href="https://scholar.google.com/citations?user=QbW4GSwAAAAJ&hl=en">Google Scholar</a> | <a href="https://x.com/_angie_chen">Twitter</a>
</p>
Hi! I'm a PhD student at NYU Center for Data Science in the [Machine Learning for Language](https://wp.nyu.edu/ml2/) group, advised by [Kyunghyun Cho](https://kyunghyuncho.me/). My research primarily focuses on understanding and improving how large language models (LLMs) learn from <b>online feedback</b> through the lens of <b>training dynamics</b>. Recently, I've also become interested in the use of <b>LLMs for biological applications</b>.

I have worked as a PhD intern at Google Research on [streaming models](https://arxiv.org/abs/2205.00620), as a student researcher at Google Brain on [evolution with LLMs](https://arxiv.org/abs/2302.14838), and as a research intern at Prescient Design on LLMs for discrete sequence optimization. Prior to NYU, I worked as a SWE at Google and graduated with high honors from Princeton Computer Science. [Sebastian Seung](https://www.cs.princeton.edu/people/profile/sseung) advised my senior thesis at Princeton, for which I received an Outstanding Computer Science Thesis award.

Outside of my research, I enjoy running and baking more pastries than I can feasibly eat. I also volunteer as a rape and domestic violence crisis counselor/victim advocate for the [NYC Crime Victims Treatment Center](https://www.cvtcnyc.org/) and a crisis counselor for [Crisis Text Line](https://www.crisistextline.org/).

<h2 align="center" id="news">
  News
</h2>

* * *
* <b>I am currently on the job market!</b> I'm primarily looking for full-time research scientist or postdoc positions (in industry or academia) focused on preference learning, LLM training dynamics, or LLMs for biological applications.
* I gave a recent plenary talk at the ICML 2024 Workshop for High-Dimensional Learning Dynamics (HiLD) on the kinds of misleading conclusions we might come to if we neglect to analyze LLM training dynamics. Video is <a href="https://icml.cc/virtual/2024/workshop/29974">here</a> (requires an ICML login, starts at ~52:00) and slides are <a href="https://docs.google.com/presentation/d/1ibVtU2Ug3OCbeo1X8VObjGatBPMS62Y1h_L1s05IsvY/edit?usp=sharing">here</a>.
* As of fall 2024, I am also a <b>Visiting Researcher</b> (20% time) at <b>Meta AI NYC</b> and a part-time <b>ML Scientist</b> (20% time) at <b>Prescient Design</b>.


<h2 align="center" id="papers">
  Selected Papers
</h2>

* * *

My work is largely split into three general directions -- understanding LLM training, improving how LLMs learn from feedback, and evaluating LLMs. For a more complete list of my papers, please see <a href="https://www.semanticscholar.org/author/Angelica-Chen/13336152">Semantic Scholar</a>.<br>

<h3>Understanding LLM Training</h3>
<b><u>Preference Learning Algorithms Do Not Learn Preference Rankings</u></b><br>
<i>NeurIPS 2024</i><br>
<i>Oral at ICML 2024 Workshop on Models of Human Feedback for AI Alignment (MHFAIA)</i><br>
<b>Chen, Angelica</b>, Sadhika Malladi, Lily H. Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, Kyunghyun Cho.<br>
[<a href="https://arxiv.org/abs/2405.19534">Arxiv</a>] [<a href="https://github.com/angie-chen55/pref-learning-ranking-acc/tree/main">GitHub</a>] <br>
<br>
<b><u>Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs</u></b><br>
<i>ICLR 2024 (Spotlight)</i><br>
<b>Chen, Angelica</b>, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, Naomi Saphra.<br>
[<a href="https://openreview.net/forum?id=MO5PiKHELW">OpenReview</a>] [<a href="https://arxiv.org/abs/2309.07311">Arxiv</a>] <br>
<br>
<b><u>Latent State Models of Training Dynamics</u></b><br>
<i>Transactions on Machine Learning Research</i><br>
Michael Y. Hu, <b>Angelica Chen</b>, Naomi Saphra, Kyunghyun Cho<br>
[<a href="https://arxiv.org/abs/2308.09543">Arxiv</a>] [<a href="https://openreview.net/forum?id=NE2xXWo0LF&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DTMLR%2FAuthors%23your-submissions)">OpenReview</a>]<br>
<br>

<h3>Improving How LLMs Learn From Feedback</h3>
<b><u>LLMs are Highly-Constrained Biophysical Sequence Optimizers</u></b><br>
<i>Preprint</i><br>
<i>(Spotlight) NeurIPS 2024 Workshop on AI for New Drug Modalities (AIDrugX)</i><br>
<b>Angelica Chen</b>, Samuel D. Stanton, Robert G. Alberstein, Andrew M. Watkins, Richard Bonneau, Vladimir Gligorijevi, Kyunghyun Cho, Nathan C. Frey<br>
[<a href="https://arxiv.org/abs/2410.22296">Arxiv</a>]
<br><br>
<b><u>Playing Large Games with Oracles and AI Debate</u></b><br>
<i>Preprint</i><br>
Xinyi Chen, <b>Angelica Chen</b>, Dean Foster, Elad Hazan.<br>
[<a href="https://arxiv.org/abs/2312.04792">Arxiv</a>] [<a href="https://github.com/angie-chen55/alignment-game-public">GitHub</a>] <br>
<br>
<b><u>EvoPrompting: Language Models for Code-Level Neural Architecture Search</u></b><br>
<i>NeurIPS 2023 (poster)</i><br>
<b>Chen, Angelica</b>, David M. Dohan and David R. So<br>
[<a href="https://openreview.net/forum?id=ifbF4WdT8f">OpenReview</a>] [<a href="https://arxiv.org/abs/2302.14838">Arxiv</a>]<br>
<br>
<b><u>Learning from Natural Language Feedback</u></b><br>
<i>Transactions on Machine Learning Research</i><br>
<b>Chen, Angelica<sup>\*</sup></b>, Jérémy Scheurer<sup>\*</sup>, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, Ethan Perez<br>
[<a href="https://openreview.net/forum?id=xo3hI5MwvU">OpenReview</a>] [<a href="https://github.com/nyu-mll/ILF-for-code-generation">GitHub</a>]<br>
<br>
<b><u>Pretraining Language Models with Human Preferences</u></b><br>
<i>ICML 2023 (oral)</i><br>
Korbak, Tomasz, Kejian Shi, <b>Angelica Chen</b>, Rasika Bhalerao, Christopher L. Buckley, Jason Phang, Sam Bowman and Ethan Perez<br>
[<a href="https://arxiv.org/abs/2302.08582.pdf">Arxiv</a>]<br>
<br>
<b><u>Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection</u></b><br>
<i>NAACL 2022 (oral)</i><br>
<b>Chen, Angelica</b>, Victoria Zayats, Daniel David Walker and Dirk Ryan Padfield<br>
[<a href="https://www.aclanthology.org/2022.naacl-main.60.pdf">ACL Anthology</a>]<br>
<br>

<h3>Evaluating LLMs</h3>
<b><u>Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs</u></b><br>
<i>Transactions on Machine Learning Research</i><br>
<b>Chen, Angelica</b>, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao, Samuel R. Bowman, Kyunghyun Cho.<br> 
[<a href="https://arxiv.org/abs/2305.14279">Arxiv</a>] [<a href="https://openreview.net/forum?id=5nBqY1y96B">OpenReview</a>]<br>
<br>
<b><u>QuALITY: Question Answering with Long Input Texts, Yes!</u></b><br>
<i>NAACL 2022</i><br>
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, <b>Angelica Chen</b>, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, Samuel Bowman<br>
[<a href="https://aclanthology.org/2022.naacl-main.391/">ACL Anthology</a>]<br>
<br>
<b><u>BBQ: A hand-built bias benchmark for question answering</u></b><br>
<i>ACL Findings 2022</i><br>
Parrish, Alicia, <b>Angelica Chen</b>, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut and Sam Bowman<br>
[<a href="https://aclanthology.org/2022.findings-acl.165/">ACL Anthology</a>]<br>

<h2 align="center" id="talks">
  Invited Talks
</h2>

* * *

*   "Misleading Endpoints – Lessons from LLM Training Dynamics" (Plenary) <i><a href="https://icml.cc/virtual/2024/workshop/29974">ICML 2024 Workshop on High-Dimensional Learning Dynamics (HiLD)</a></i>, July 26, 2024
*   "Preference Learning Algorithms Do Not Learn Preference Rankings," <i><a href="https://icml.cc/virtual/2024/workshop/29943">ICML 2024 Workshop on Models of Human Feedback for AI Alignment</a></i>, July 26, 2024
*   "Interpreting Model Training," <i><a href="https://pli.princeton.edu/events/seminar-lunch-series">Princeton Language and Intelligence Lunch Seminar Series</a></i>, April 18, 2024 (<a href="https://docs.google.com/presentation/d/1zyjPLBF1RyGCIxWPs_G81YdXAP2rgaCFksraiUZSDr0/edit?usp=sharing">Slides</a>)
*   "Sudden Drops in the Loss," <i><a href="https://mechinterp.com/reading-group">Mechanistic Interpretability Reading Group</a></i>, October 25, 2023
*   "Learning From Natural Language Feedback," <i>Cohere</i>, August 2, 2023 (<a href="https://www.youtube.com/watch?v=Ex2qCbZCIFI">Video</a>)
*   "Learning From Natural Language Feedback," <i>Mosaic ML</i>, April 16, 2023
*   "Teaching BERT To Wait," <i>NYU Natural Language Understanding DS-GA 1012/LING-GA 1012</i>, March 10, 2022 (<a href="https://docs.google.com/presentation/d/1kDJU-Ar03UWzeXlu4RzF04tFXc6v2ei6GV6VYqLvPus/edit?usp=sharing">Slides</a>)
